<!DOCTYPE html>
<html lang="en">

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FTK67N3EL9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FTK67N3EL9');
</script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Sinho Chewi's Website</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

{% block body %}
<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="/">Home</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#page-top"></a>
                    </li>
                    </li>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Main Section -->
    <section id="sds605" class="course-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <center><h1>S&DS 432/632: Advanced Optimization Techniques (Spring 2025)</h1></center>
                    <center><p><b>Course Description:</b> This course covers fundamental optimization algorithms and their theoretical analysis, emphasizing convex optimization. Topics covered include gradient descent and acceleration; lower bounds; structured problems; Newton's method; and interior point methods. Prerequisites: Knowledge of linear algebra, such as MATH 222/225; multivariate calculus, such as MATH 120; probability, such as S&DS 241/541; optimization, such as S&DS 431/631; and comfort with proof-based exposition and problem sets.</p></center>
                    <center><p><b>Instructor:</b> Sinho Chewi (sinho.chewi@yale.edu)</p></center>
                    <h3>References</h3>
                    <ul>
                        <li><b><a href="opt_notes.pdf">Lecture notes.</a></b></li>
                        <li><a href="https://arxiv.org/abs/1405.4980">[B] S. Bubeck, <i>Convex optimization: algorithms and complexity</i> (2015).</a></li>
                        <li><a href="https://link.springer.com/book/10.1007/978-3-319-91578-4">[N] Y. Nesterov, <i>Lectures on convex optimization</i> (2018).</a></li>
                        <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">[NN] Y. Nesterov and A. Nemirovskii, <i>Interior-point polynomial algorithms in convex programming</i> (1994).</a></li>
                    </ul>

                    <h3>Schedule</h3>
                    <p>The course meets on Tuesdays and Thursdays, 1–2.15 p.m., in Kline Tower 203. I will also be available for an hour after each class if you have any questions or if you would like to discuss. If these times do not work for you, or if you have any other concerns, please reach me via email.</p>
                    <ul>
                        <li>Jan. 14 (Tuesday): Overview of the course, preliminaries on convexity and smoothness.</li>
                        <li>Jan. 16 (Thursday): Gradient flow.</li>
                        <li>Jan. 21 (Tuesday): Gradient descent: smooth case.</li>
                        <li>Jan. 23 (Thursday): Reductions and lower bounds for smooth convex optimization.</li>
                        <li>Jan. 28 (Tuesday): Acceleration for quadratics: conjugate gradient.</li>
                        <li>Jan. 30 (Thursday): Acceleration in continuous time.</li>
                        <li>Feb. 4 (Tuesday): Acceleration in discrete time. Convex analysis.</li>
                        <li>Feb. 6 (Thursday): Convex analysis and projected subgradient methods.</li>
                        <li>Feb. 11 (Tuesday): Projected subgradient methods and cutting plane methods.</li>
                        <li>Feb. 13 (Thursday): Lower bounds for non-smooth convex optimization.</li>
                        <li>Feb. 18 (Tuesday): Lower bounds continued. Frank–Wolfe.</li>
                        <li>Feb. 20 (Thursday): Proximal methods.</li>
                        <li>Feb. 25 (Tuesday): Convergence of proximal methods. Fenchel duality.</li>
                        <li>Feb. 27 (Thursday): Fenchel duality.</li>
                        <li>Mar. 4 (Tuesday): Mirror methods.</li>
                        <li>Mar. 6 (Thursday): Mirror methods.</li>
                        <li>Mar. 25 (Tuesday): Mirror methods and online learning.</li>
                        <li>Mar. 27 (Thursday): Alternating minimization.</li>
                        <li>Apr. 1 (Tuesday): Alternating minimization.</li>
                        <li>Apr. 3 (Thursday): Sinkhorn's algorithm for entropic optimal transport.</li>
                        <li>Apr. 8 (Tuesday): Stochastic optimization and generalization.</li>
                        <li>Apr. 10 (Thursday): Central limit theorem for averaged SGD.</li>
                    </ul>
                    <h3>Assignments</h3>
                    <p>Grades are based solely on six problem sets and one take-home final exam. Tentatively, the problem sets will be released on 1/21, 2/4, 2/18, 3/4, 3/25, and 4/8, and each one is due roughly two weeks after it is assigned. For each problem set, the last problem is mandatory for students taking S&DS 632, and extra credit for all other students. Each problem set is weighted equally with the final, so each count for 1/7 of the total grade.</p>
                    <p>Please feel free to work on the problem sets with others, but list your collaborators at the beginning of your submission. Solutions should be arrived at without the use of AI. Problem sets should be typed in LaTeX and submitted via Gradescope (the link can be found in Canvas).</p>
                </div>
            </div>
        </div>
    </section>

</body>
{% endblock %}

</html>
